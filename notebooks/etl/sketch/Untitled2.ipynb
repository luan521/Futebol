{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932ce77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from cafu.metadata import path\n",
    "path_odds = path('dir_results')+'/odds/campeonato=franca_index=1.json'\n",
    "r = open(path_odds, 'r')\n",
    "odds = json.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9497e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/19 13:50:39 WARN Utils: Your hostname, luan-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/03/19 13:50:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/spark-3.0.3-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/luan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/luan/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark/spark-3.0.3-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-29173468-4ba2-4fb4-a187-eba1662f39fa;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 894ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-29173468-4ba2-4fb4-a187-eba1662f39fa\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/25ms)\n",
      "22/03/19 13:50:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/19 13:50:46 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.\n",
      "java.lang.NoClassDefFoundError: org/apache/spark/sql/catalyst/SQLConfHelper\n",
      "\tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\n",
      "\tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)\n",
      "\tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:174)\n",
      "\tat java.base/java.net.URLClassLoader.defineClass(URLClassLoader.java:555)\n",
      "\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:458)\n",
      "\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:452)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:451)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\n",
      "\tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)\n",
      "\tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:174)\n",
      "\tat java.base/java.net.URLClassLoader.defineClass(URLClassLoader.java:555)\n",
      "\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:458)\n",
      "\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:452)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:451)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat io.delta.sql.DeltaSparkSessionExtension.apply(DeltaSparkSessionExtension.scala:84)\n",
      "\tat io.delta.sql.DeltaSparkSessionExtension.apply(DeltaSparkSessionExtension.scala:73)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$1(SparkSession.scala:1163)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$1$adapted(SparkSession.scala:1158)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1158)\n",
      "\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:101)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.catalyst.SQLConfHelper\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\t... 40 more\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "from cafu.utils import get_spark\n",
    "spark = get_spark(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd1fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cafu.utils import proximas_partidas\n",
    "from cafu.metadata import campeonato_dafabet\n",
    "from cafu.queries import GetOdds\n",
    "from datetime import datetime, date, timedelta\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df36a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# campeonatos que terão jogos, hoje ou amanhã\n",
    "pr_part = proximas_partidas()\n",
    "dafabet = campeonato_dafabet()\n",
    "after_tomorrow = date.today()+timedelta(days=2)\n",
    "campeonatos = []\n",
    "for c in pr_part:\n",
    "    for t in pr_part[c]:\n",
    "        try:\n",
    "            prox_jogo = datetime.strptime(pr_part[c][t], '%Y-%m-%d').date()\n",
    "            if (prox_jogo<=after_tomorrow) and (c in dafabet):\n",
    "                campeonatos.append([c,t])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c619f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:05<00:49,  5.52s/it]\n"
     ]
    }
   ],
   "source": [
    "query = GetOdds()\n",
    "for c in campeonatos:\n",
    "    stop = False\n",
    "    index = 0\n",
    "    while not stop:\n",
    "        query.get_campeonato_dafabet(c[0])\n",
    "        descricao_partida = query.join_link_odds_partida(index)\n",
    "        if descricao_partida is not None:\n",
    "            if (\n",
    "                ('Hoje' in descricao_partida['horario']) or\n",
    "                ('Amanhã' in descricao_partida['horario'])\n",
    "               ):\n",
    "                query.open_odds()\n",
    "                odds = query.get_odds()\n",
    "                for k1 in odds:\n",
    "                    for k2 in odds[k1]:\n",
    "                        odds[k1][k2] = float(odds[k1][k2])\n",
    "                for k1 in odds:\n",
    "                    df = spark.createDataFrame([odds[k1]])\n",
    "                    df = (\n",
    "                             df\n",
    "                             .withColumn('time_casa', F.lit(descricao_partida['time_casa']))\n",
    "                             .withColumn('time_visitante', F.lit(descricao_partida['time_visitante']))\n",
    "                             .withColumn('horario', F.lit(descricao_partida['horario']))\n",
    "                             .withColumn('campeonato_metadata', F.lit(c))\n",
    "                             .withColumn('temporada_metadata', F.lit(t))\n",
    "                             .withColumn('date_update', F.lit(datetime.now()))\n",
    "                         )\n",
    "                    for c in df.columns:\n",
    "                        df = df.withColumnRenamed(c, (c.replace(' ','_')\n",
    "                                                       .replace(',','_'))\n",
    "                                                 )\n",
    "                    dir_ = k1.replace('/','-')\n",
    "                    dir_ = unidecode(dir_.lower())\n",
    "                    df.write.parquet(f'teste/{dir_}', mode='append')\n",
    "            else:\n",
    "                stop = True\n",
    "        else:\n",
    "            stop = True\n",
    "        index+=1\n",
    "query.web.close() # encerra a sessão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6571f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = odds, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e0c98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "futebol",
   "language": "python",
   "name": "futebol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
